---
title: "MY474 - Problem Set 5"
date: "29 March 2023"
output: html_document
---

This is an individual assignment.


__Exercise 1: Engineering missing value indicator features__

In certain setups with tabular data that are commonly encountered in research and business cases, you will find that engineered features can potentially improve performance in prediction much more than the choice of the algorithm itself. In these cases, data scientists therefore often spend a lot of time carefully thinking about the set of features. This and the following exercise will study the topic in more detail.

The exemplary dataset we are going to look at is a sample of housing data from California. One line/observation in this dataset is a block. The task is to build a model that predicts the variable `median_house_value` as well as possible. You can find the data in the files `housing_train.csv` and `housing_test.csv`.

Loading packages:

```{r}
library("tidyverse")
library("glmnet")
```

Loading the data:

```{r}
train_data <- read_csv("data/housing_train.csv")
test_data <- read_csv("data/housing_test.csv")
```


In our example here, some features in the training and test datasets have missing values. In research and work as a data scientist this is a common case. One option is to drop all rows with one or more NA values, however, this also gets rid of a lot of information from the other columns (which can be particularly problematic if the dataset is small to begin with). In this exercise we are trying to use all information in the data and deal with missing values well. 

There are many approaches to impute missing values, with the simplest being to choose the column mean (continuous variable) or mode (categorical variable). More important than the imputation technique itself (!), however, is often to signal to the model that a value has been imputed at all. This is what we are going to study.

a) Impute missing values in the continuous features of `train_X` and `test_X` with their column mean from `train_X.` This assumes that new test data will come in at high frequency in later implementations of the model, and that it will not be possible to recompute the means including also the test data every time. For missing values in the categorical variable, replace them with the mode of the variable in the training data. Next, using the training data, cross validate a LASSO (note: using the function `lasso_model <- cv.glmnet(...)` does both in one - it chooses the best lambda via cross validation and can also be used to predict afterwards). What is the RMSE that this model achieves on the test data?

To answer this question, read through the code to make sure to understand it and just fill in the blanks: (1.5 points)


```{r}
# Helper function for getting the mode in R (source: https://stackoverflow.com/questions/2547402/how-to-find-the-statistical-mode)
get_mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

# Because the following imputation is only a reference, we will wrap the code
# into a function such that we can run it and see the associated RMSE but avoid
# having many additional objects stored in the global environment afterwards
simple_imputation <- function() {
  
  # Copies of raw data
  train <- train_data
  test <- test_data
  
  # Computing means using only training data
  means <- train %>% select(population, median_income) %>% colMeans(na.rm = TRUE)
  
  # Imputing missing values of continuous features with train means
  train[is.na(train$population), "population"] <- means["population"]
  train[is.na(train$median_income), "median_income"] <- means["median_income"]
  test[is.na(test$population), "population"] <- means["population"]
  test[is.na(test$median_income), "median_income"] <- means["median_income"]
  
  # Adding the mode for missing categorical variable values
  train[is.na(train$ocean_proximity), "ocean_proximity"] <- get_mode(train$ocean_proximity)
  test[is.na(test$ocean_proximity), "ocean_proximity"] <- get_mode(train$ocean_proximity)
  
  # training and test X/y
  train_X <- model.matrix(~., train %>% select(-median_house_value))
  test_X <- model.matrix(~., test %>% select(-median_house_value))
  
  # Why do we use `model.matrix` to compute the input for the glmnet's LASSO?
  # It transforms categorical levels into 0/1 and and also adds an intercept column
  
  # Train/test y's
  train_y <- train %>% pull(median_house_value)
  test_y <- test %>% pull(median_house_value)
  
  # Train lasso model
  lasso_model <- cv.glmnet(x = train_X, y = train_y)
  
  # Predict train_y_hat and test_y_hat
  train_y_hat <- predict(lasso_model, train_X)
  test_y_hat <- predict(lasso_model, test_X)
  
  # Test RMSE
  print(sqrt(mean((test_y - test_y_hat)^2)))
  
}

# Running simple imputation
paste("The RMSE for this model is:" , simple_imputation())
```


b) The approach above is a naive way to deal with missing values. Our first engineered features will therefore be new columns with missing value indicators. Add one indicator column to both `train_X` and `test_X` for each _continuous_ variable with missing values. These indicator columns take a value of 1 if the value in a corresponding variable was imputed and zero otherwise. Conveniently, for each categorical variable, you can just add a new label if an observation is missing (instead of using the mode). Train a model with the new `train_X` matrix containing the indicators and predict with the new `test_X` matrix containing the indicators. 

To answer this question, again fill in the blanks in the following code. What test RMSE do you find now? Lastly, when do you think will adding such indicator columns be most important? (1.5 points)


```{r}
# Copies of raw data
train <- train_data
test <- test_data

# Computing means using only training data
means <- train %>% select(population, median_income) %>% colMeans(na.rm = TRUE)

# Adding missing value indicators for continuous features
train$population_indicator <- as.numeric(is.na(train$population))
train$median_income_indicator <- as.numeric(is.na(train$median_income))
test$population_indicator <- as.numeric(is.na(test$population))
test$median_income_indicator <- as.numeric(is.na(test$median_income))

# Imputing missing values of continuous features with train means
train[is.na(train$population), "population"] <- means["population"]
train[is.na(train$median_income), "median_income"] <- means["median_income"]
test[is.na(test$population), "population"] <- means["population"]
test[is.na(test$median_income), "median_income"] <- means["median_income"]

# Adding new labels for categorical variables
train[is.na(train$ocean_proximity), "ocean_proximity"] <- "not_available"
test[is.na(test$ocean_proximity), "ocean_proximity"] <- "not_available"

# training and test X/y
train_X <- model.matrix(~., train %>% select(-median_house_value))
test_X <- model.matrix(~., test %>% select(-median_house_value))
# Why do we use `model.matrix` to compute the input for the glmnet's LASSO?
# What does `model.matrix` do here?

# Train/test y's
train_y <- train %>% pull(median_house_value)
test_y <- test %>% pull(median_house_value)

# Train lasso model
set.seed(123)
lasso_model <- cv.glmnet(x = train_X, y = train_y)

# Predict train_y_hat and test_y_hat
train_y_hat <- predict(lasso_model, train_X)
test_y_hat <- predict(lasso_model, test_X)

# Test RMSE
print(sqrt(mean((test_y - test_y_hat)^2)))
```


**The test RMSE is 68402.31 with the indicator variable, compared to 85681.69 without the indicator variable. This is considerably lower. Adding the missing variable indicator is probably most important when a similar group of data is missing. For example, if all the median income data is missing for a minority population that might be likely to have similar incomes, it would be more important to use the missing variable indicator than if a random subset of data that follows the distribution of the datset was missing.**

__Exercise 2: General feature engineering__

Unfortunately there is no standardised approach to engineering features that always manages to improve predictions. In the following, I have therefore assembled a list of some ideas and thoughts on feature engineering. Many of these might eventually not improve a particular problem, but if you have the time it can be worth it to try these and similar approaches. It is often the combination of many small improvements which determine the best model. In business applications, small improvements in predictions can also translate into relevant financial gains (think e.g. of having slightly better predictions of prices which consumers would be willing to pay in a large web shop). Only very few of the bullet points listed below might help/be applicable for this particular problem, the purpose of the list is also to be a starting point which might be helpful for your future work.

- Is it possible to gather data on new features which could have predictive power for the respective problem? It can be worth it to first take a step back and think about what the ideal predictive feature for a particular problem could be. Would it be possible to obtain data for this feature or a similar one? You can also use measures of feature importance to get a broad overview which group of features is more predictive and whether you can find similar new data, but there is clear value in first thinking about the problem more abstractly before running such computations.

- Whether you impute missing values in continuous features just with the mean or with an advanced method, it is key to add new indicator features as we have done in Exercise 1.

- Create histograms of features, do some have very skewed distributions with outliers which might throw off predictions? Potentially try transformations such as the log for these features. Do you find that the transformed features look more normally distributed in histograms? Then it might be useful to use those. Note, however, that when you try to predict y values with a lot of outliers and only use a linear model, the outliers in the x features before any log transform can also be helpful in predicting the outlier y values. In contrast, a non-linear model could also predict y outliers well if the corresponding x values have a more normally distributed shape and indeed further benefit from less erratic feature values.

```{r}
#approx normal distr
hist(train$housing_median_age)
hist(train$median_income)

#skewed dist
hist(train$total_rooms)
hist(train$total_bedrooms)
hist(train$population)
hist(train$households)

#look at logs
hist(log(train$total_rooms))
hist(log(train$total_bedrooms))
hist(log(train$population))
hist(log(train$households))
```


```{r}
#logs looked more normal for all of them, lowers RMSE

#create log values
train$log_population <- log(train$population)
train$log_households <- log(train$households)
train$log_rooms <- log(train$total_rooms)
train$log_bedrooms <- log(train$total_bedrooms)

test$log_population <- log(test$population)
test$log_households <- log(test$households)
test$log_rooms <- log(test$total_rooms)
test$log_bedrooms <- log(test$total_bedrooms)

# only use log transformed variables in the regression
train_logs <- subset(train, select = -c(population, households, total_rooms, total_bedrooms))
test_logs <- subset(test, select = -c(population, households, total_rooms, total_bedrooms))

#rerun code

# training and test X/y
train_X <- model.matrix(~., train_logs %>% select(-median_house_value))
test_X <- model.matrix(~., test_logs %>% select(-median_house_value))

# Train/test y's
train_y <- train_logs %>% pull(median_house_value)
test_y <- test_logs %>% pull(median_house_value)

# Train lasso model
set.seed(123)
lasso_model <- cv.glmnet(x = train_X, y = train_y)

# Predict train_y_hat and test_y_hat
train_y_hat <- predict(lasso_model, train_X)
test_y_hat <- predict(lasso_model, test_X)

# Test RMSE
print(sqrt(mean((test_y - test_y_hat)^2)))
```


- Try to winsorize or remove extreme outliers in the outcome variable or features that might bias predictions on the non-outlier observations. Another approach is to log transform the outcome variable. Beware though that by Jensen's inequality your predictions will be biased if you transform the outcome variable back with exp(y_hat) afterwards. This bias might be small, however, for a given application 

```{r}
#approx normal distr
boxplot(train$housing_median_age)
boxplot(train$median_income)

#skewed dist
boxplot(train$log_rooms)
boxplot(train$log_bedrooms)
boxplot(train$log_population)
boxplot(train$log_households)
```
```{r}
#remove outliers in train data
train_room_outliers <- boxplot(train_logs$log_rooms, plot=FALSE)$out
train_bedroom_outliers <- boxplot(train_logs$log_bedrooms, plot=FALSE)$out
train_pop_outliers <- boxplot(train_logs$log_population, plot=FALSE)$out
train_house_outliers <- boxplot(train_logs$log_households, plot=FALSE)$out

#remove outliers in test data
test_room_outliers <- boxplot(test_logs$log_rooms, plot=FALSE)$out
test_bedroom_outliers <- boxplot(test_logs$log_bedrooms, plot=FALSE)$out
test_pop_outliers <- boxplot(test_logs$log_population, plot=FALSE)$out
test_house_outliers <- boxplot(test_logs$log_households, plot=FALSE)$out

#create outlier free dataframes
outlier_train <- train_logs[-which(train_logs$log_rooms %in%  train_room_outliers | train_logs$log_bedrooms %in% train_bedroom_outliers | train_logs$log_population %in% train_pop_outliers | train_logs$log_households %in% train_house_outliers),]

outlier_test <- test_logs[-which(test_logs$log_rooms %in%  test_room_outliers | test_logs$log_bedrooms %in% test_bedroom_outliers | test_logs$log_population %in% test_pop_outliers | test_logs$log_households %in% test_house_outliers ),]

# training and test X/y
train_X <- model.matrix(~., outlier_train %>% select(-median_house_value))
test_X <- model.matrix(~., outlier_test %>% select(-median_house_value))

# Train/test y's
train_y <- outlier_train %>% pull(median_house_value)
test_y <- outlier_test %>% pull(median_house_value)

# Train lasso model
set.seed(123)
lasso_model <- cv.glmnet(x = train_X, y = train_y)

# Predict train_y_hat and test_y_hat
train_y_hat <- predict(lasso_model, train_X)
test_y_hat <- predict(lasso_model, test_X)

# Test RMSE
print(sqrt(mean((test_y - test_y_hat)^2)))
```


- To minimise the negative impact of outlier observations in the outcome variable on the non-outlier observation without winsorizing or removing outliers, it can also be useful to use a Huber loss function in regression. Flexible models like LightGBM allow to swap the loss function relatively easily.

- It can be interesting to check pairwise correlations of features with the outcome variable. Are there some clear associations, potentially non-linear? Then it might be helpful to either try adding higher order terms of the feature or even discretising it into several indicator variables that change their value to 1 if the feature is in a specific range (this allows the model to fit very non-linear relations between the feature and the outcome variable).

```{r}
# check pairwise correlations
corr_train <- select_if(outlier_train, is.numeric)
cor(corr_train)
```


```{r}
# square age, as relations are often non-linear with age
outlier_train$age_sq <- outlier_train$housing_median_age^2
outlier_test$age_sq <- outlier_test$housing_median_age^2

# create interaction variables with income bc income was the highest correlated var with the outcome
outlier_train$income_pop <- outlier_train$median_income * outlier_train$log_population
outlier_test$income_pop <- outlier_test$median_income * outlier_test$log_population

outlier_train$income_house <- outlier_train$median_income * outlier_train$log_households
outlier_test$income_house <- outlier_test$median_income * outlier_test$log_households

outlier_train$income_age <- outlier_train$median_income * outlier_train$housing_median_age
outlier_test$income_age <- outlier_test$median_income * outlier_test$housing_median_age

outlier_train$income_agesq <- outlier_train$median_income * outlier_train$age_sq
outlier_test$income_agesq <- outlier_test$median_income * outlier_test$age_sq

outlier_train$income_room <- outlier_train$median_income * outlier_train$log_rooms
outlier_test$income_room <- outlier_test$median_income * outlier_test$log_rooms

outlier_train$income_bedroom <- outlier_train$median_income * outlier_train$log_bedrooms
outlier_test$income_bedroom <- outlier_test$median_income * outlier_test$log_bedrooms

# training and test X/y
train_X <- model.matrix(~., outlier_train %>% select(-median_house_value))
test_X <- model.matrix(~., outlier_test %>% select(-median_house_value))

# Train/test y's
train_y <- outlier_train %>% pull(median_house_value)
test_y <- outlier_test %>% pull(median_house_value)

# Train lasso model
set.seed(123)
lasso_model <- cv.glmnet(x = train_X, y = train_y)

# Predict train_y_hat and test_y_hat
train_y_hat <- predict(lasso_model, train_X)
test_y_hat <- predict(lasso_model, test_X)

# Test RMSE
print(sqrt(mean((test_y - test_y_hat)^2)))
```

- Another approach is more agnostic, namely to add a range of transformations to the data e.g. squares and interactions (try e.g. `model.matrix( ~.^n, data = some_data)` which creates interactions of order `n` or the function [step_interact](https://recipes.tidymodels.org/reference/step_interact.html)). After you have blown up the number of columns in feature matrix, train e.g. a LASSO or other method that can deal well with many columns. Caveat: Recall that the combinatorics implied by even a very small number `n` can create a very large amount of new columns which can become difficult to handle for the computer's memory and greatly increase training time. Furthermore, higher order terms can behave quite erratically, so it is important to thoroughly test such models using the new transformation on out of sample data.

```{r}
# create interactions of order 2
# tested this + manual interactions, together they lowered error more

# training and test X/y
train_X <- model.matrix(~.^2, outlier_train %>% select(-median_house_value))
test_X <- model.matrix(~.^2, outlier_test %>% select(-median_house_value))

# Train/test y's
train_y <- outlier_train %>% pull(median_house_value)
test_y <- outlier_test %>% pull(median_house_value)

# Train lasso model
set.seed(123)
lasso_model <- cv.glmnet(x = train_X, y = train_y)

# Predict train_y_hat and test_y_hat
train_y_hat <- predict(lasso_model, train_X)
test_y_hat <- predict(lasso_model, test_X)

# Test RMSE
print(sqrt(mean((test_y - test_y_hat)^2)))
```


- When having high-dimensional categoricals, it is usually the case that many of the levels have very few observations. To make training faster and also avoid over-fitting on levels with very few observations, it can be worth it to pool rare levels into an "other" category. This also makes it easier when encountering new observations that have an unknown but rare level in this column and which can then just be assigned the value "other" when using it in a prediction.

- Do you have alternative data sources such as texts, e.g. when predicting the price of a good with an additional product description? You can either apply domain knowledge and extract terms with regular expressions that are relevant to a specific problem (e.g. brands in descriptions of clothes, "gmail" vs "hotmail" email addresses signalling different types of customers, etc.) or take a more agnostic approach and create a document term matrix from the text, normalise it by document length, and add it to your features (yet this would make the problem very high dimensional as every unique word would become an additional feature). Furthermore, when working with texts, also other representations of terms can be helpful such as (pre-trained) word embeddings which can be freely downloaded for most words or context specific word embeddings obtained with transformers.

Using your __LASSO model (it is not allowed in this exercise to user other algorithms)__ built previously including the missing value indicators, modify the code and present a few potential transformations etc. to engineer features for this dataset. Why did you try these in particular?

Could you further bring down the test RMSE predicting `median_house_value` now also using further engineered features? If you could not reduce the test RMSE further from relative to Exercise 1 b), what could be interesting data to collect here? Please clearly report the best test RMSE value you could achieve. (5 points)

**I was able to lower the test RMSE to 60505.94, down from 68402.31. I did this by: log transforming variables with non-normal distributions, removing outliers, manually creating interaction terms based on theory and an understanding of the data set, and then simply adding a range of transformations by creating an interaction term between each variable, and squaring each variable. It's important to note that the transformations I made built upon each other.**

**I tried log transformations on the following variables (population, households, rooms, and bedrooms) because the histograms for these variables were right skewed, and their log transformations looked more normally distributed. I removed outliers on the same 4 variables because their boxplots showed many outliers, and removing them decreased the RMSE, indicating that the outliers were biasing the model. I created interaction terms after checking the pair-wise correlations between all the variables and the outcome variable. I found that median income was the most correlated with household value, so I chose to make interaction variables between income and all the other predictors.**

**The last transformation (creating a range of terms) had the largest impact on lowering the RMSE. However, I believe it's important to first try to reduce error by using methods that are rooted in theory such as log transforming the variables with non-normal distributions and removing outliers. I found that using the range of transformations in conjunction with the manually created interaction terms helped reduce RMSE by an additional ~1,000, even though the range creates interaction terms between each variable. The existing presence of the interaction terms clearly created a feature that was a helpful predictor.**

__Exercise 3__

Much discussion about machine learning and AI over the recent months has been driven by large language models (LLMs). At the moment, these models are typically decoder transformers of the kind that we discussed in the lecture. For now the GPT models such as ChatGPT are most popularised and able to follow instructions quite well, but there are several models from other providers available and more will be released in the upcoming months and years.

There likely exist a multitude of potential research projects in the social sciences where such models could be utilised. For some social science research involving texts, you can e.g. think of an LLM like an electronic research assistant that can process texts very well and at scale, summarise them, extract information from them, etc. You could repeatedly submit short to medium length texts to the model and ask questions about these. For example, a query to the model might look like:

"[Some excerpt from an economic text pasted here, e.g. from a newspaper or company report]

Yes or no, does this text deal with environmental issues? If yes, summarise the environmental issues in less than 200 words. Lastly, is the sentiment of the text likely positive, negative, or neutral?"

The response text generated by the model will contain a sentiment label and potentially also a summary. These pieces of information could be separated and stored in a row of a tabular dataset (e.g. to then analyse subsequently with simpler methods). Afterwards the next newspaper text/company report could be parsed into the model with the same question added below it, and so on. In the end, a dataset assembled from such prompts might e.g. be used in a wider research paper analysing environmental discussions in company reports and how they changed over time.

This exercise is meant as a space for you to brainstorm and develop ideas how LLMs could be used in a research project in your field of the social sciences. Write a short research proposal/extended abstract of __up to 600 words__ where you try to develop a research idea. This text is __not meant to be only generic__ like "One could use LLMs in financial economics research", but instead meant to describe a proposed research idea/project as precisely as possible given the word limit. Make sure to cite relevant literature and sources where appropriate and list them in the references. While developing the idea, you might further read into large language models. One important particularity of these models to recall from the lecture is their maximum context window length -- it is so far not possible to parse very long texts into them in a single query and have them understand all text at once. So a research idea containing "I paste the following book that contains 2000 pages into the model as only a single combined input text and ask it to summarise this text" would not be feasible. Reading up on these and other limitations will make sure that your short research proposal remains realistic in terms of what these models currently can and also cannot do.

Begin the answer by stating a research question. As part of your answer, you can also e.g. mention potential prompts, how their answers by the LLM could be used for assembling a dataset for the project, short details of the subsequent analysis, for which tasks simpler approaches also covered in this course might be preferable (e.g. for plain sentiment analysis, a simpler tool might be almost as good, but much faster and cheaper then sending each text to an LLM), or which current limitations of LLMs could be relevant for the research project. Lastly, try to develop a new idea in this exercise rather than one related to another project you are/have been working on, but if it relates to any other of your work, make sure to cite it clearly in the references. (7 points)

More original and concrete answers that display more detailed understanding of methods discussed in this course will receive more points here. Also see the marking criteria at https://lse-my474.github.io/ for further information. 

**Research Question: Did the Black Lives Matter movement have a meaningful, and sustained impact on how companies approach Diversity, Inclusion, and Ethics (DEI)?**

Recently, the world has faced an influx of consumers interested in corporate social responsibility (CSR) (Barman 2016). This means that consumers are increasingly interested in a company’s impact on things like the environment and inequality. CSR efforts stem from commitment to “zero-waste” policies, community engagement, and increased diversity (Kim and Schifeling, 2016). As is apparent by the date on these articles, CSR is not a recent concept. However, it received more attention in 2020, as a result of the murder of George Floyd and the rise of the Black Lives Matter (BLM) movement in America (Purtell and Kang 2022). 

The movement was a turning point for American companies, which were forced to reckon with systemic racial inequality that capitalism had contributed to. Companies were pressured by consumers to make a commitment to increasing diversity, equity, and inclusion (DEI) in their company. These commitments were demonstrated in two main ways: social media posts and public company documents. Existing literature using classic machine learning text algorithms to analyze company social media posts and consumer responses on BLM posts (Yang et. al., 2021, and Purtell and Kang 2022). However, LLM models could become extremely helpful here by analyzing a corpus of company documents before and after BLM to ascertain whether or not a company was already focused on CSR before BLM, whether or not they made a commitment to increasing DEI (and to what extent), and understand whether or not the company has kept up with the commitment that they made 3 years later, as the spotlight on DEI has faded.

The analysis would focus on Fortune 500 company's that were set up 3 years before BLM (2017) and still exist today. I use Fortune 500 because previous literature on this subject focuses on those companies. The “before” functionality will allow us to have a "control" group of companies that have always been committed to CSR, and compare them to companies that only became interested in CSR as a response to BLM. Using the LLM, we can feed shorter, quarterly reports to the model to ascertain whether or not the report contains indicators of DEI initiatives. We can also ask the model whether or not those initiatives appear consistent over time and if they mention progress to their commitments.

An ideal analysis for this would be a difference-in-difference (DID) setup. The control group would be companies that have always been committed to DEI. There would be three time periods to evaluate companies pre BLM, during BLM, and post BLM to understand the extent to which they kept their promises (indicator variable generated by the LLM). Various company demographic and control factors would be inputted. The outcome variable could be profit or yearly/quarterly earnings depending on if that information is readily available for all companies in the dataset. Another outcome variable, following the analysis by Yang et. al., could be consumer engagement (measured via social media).

As previous literature has mentioned, more traditional machine learning methods could be used for parsing social media posts, to create a simple binary indicator of whether or not the brand engaged with the BLM movement at the time at all. One limitation of current LLM models as applied to this research proposal is that company documents can be lengthy, which would limit our ability to use LLMs. However, most companies release full and condensed versions of public documents, which would make this analysis possible with the current LLM capabilities, just not as powerful as it could be.

*References:*

Barman, E. (2016). Caring Capitalism. Cambridge University Press.

Kim, S., & Schifeling, T. (2016). Varied Incumbent Behaviors and Mobilization for New Organizational Forms: The Rise of Triple-Bottom Line Business amid Both Corporate Social Responsibility and Irresponsibility. SSRN. http://dx.doi.org/10.2139/ssrn.2794335

Purtell, R. E., & Kang, K. K. (2022). The Corporate Social Responsibility of Fortune 500 Companies to Black Lives Matter: Strategic Responses on Instagram. Communication Reports, 35(2), 120-133. https://doi.org/10.1080/08934215.2022.2040559

Yang, J., Chuenterawong, P., & Pugdeethosapol, K. (2021). Speaking Up on Black Lives Matter: A Comparative Study of Consumer Reactions toward Brand and Influencer-Generated Corporate Social Responsibility Messages. Journal of Advertising, 50(5), 565-583. https://doi.org/10.1080/00913367.2021.1984345


Note: For a bit of background how one would actually send texts to such a model in a research project -- For now, many LLMs cannot be run locally on computers because they are too large, but instead are commonly used via browsers. In a research project, however, it would be much easier to query the underlying model via an API (Application Programming Interface; if you are interested in the topic, have a look at the recording of week 8 in MY472) which are also often offered by the providers. In a nutshell, querying the model via an API means to send the input text with a programming language such as R or Python over the internet and receive the answer of the model (e.g. the response text or a numerical embedding of the text -- the LLMs can also return only embeddings that could be used for own downstream tasks!) back directly in the programming language. This allows to send many smaller texts to the model via loops and to process their returns directly in the programming language rather than using a visual user interface in a browser.


__Exercise 4__

Use the cifar data set from the lecture coding example `02-cnn.Rmd`. Keeping the training and test samples exactly the same as in the lecture code, can you achieve a higher accuracy than what we found in the lecture? The purpose of this exercise is to independently study some interesting concepts in neural networks which can increase predictive performance. You might e.g. want to look into topics such as batch normalisation, dropout (note: it is generally not advisable to use dropout in the convolutional layers), or other forms of regularisation. Furthermore, you can try different combinations of layers, amounts of filter, the kernel sizes of the individual filters, different numbers of training episodes (while the training loss decreases, did the validation loss already increase?), etc. Another approach is to explore alternative model architectures altogether which might improve the performance for the cifar task as well. You can find many alternative models here: https://github.com/rstudio/keras/tree/master/vignettes/examples.

Which __test set__ accuracy does your best model achieve? Which changes could improve the model? (5 points)

**I tried the following changes, in this order. Please note that due to computational complexity, I shortened the epochs to 5 from 10 (in the lecture) so that it would run in a reasonable amount of time. I have increased the epochs back to 10 for the 3 iterations that I demonstrate incremental improvement in. Additionally due to computational constraints, I have commented out the code for the iterations which did not improve accuracy.:**

- **Activation function:** RELU, ELU, and Sigmoid, of these ELU performed the best. I hypothesize that this is because the ELU function allows for negative values and is derivable at all points of the function (RELU has a sharp point at x=0 which cannot be differentiated and RELU does not allow for negative values). Therefore, it's possible that this dataset's complexity benefited from the ability to have negative values.

- **Size of Kernels:** The kernel size I began with, using the model from lecture, had kernels of size (3,3). I tried kernel sizes smaller (2,2), bigger (4,4) and (5,5), as well as asymmetrical (3,5) and (5,3). None of the kernel sizes performed better than (3,3).

- **Number of Layers:** I tried 3 layers (the current model) and 4 layers. The model performed better with 3 layers.

- **Number of Filters:** I tried increasing the number of filters in the third layer to 128, and decreasing it to 32, both iterations were worse than the current iteration of 64.

- **Optimization function:** Adam, RMSprop, and Adagrad, all different forms of gradient descent. It's unclear why RMSprop performed better than Adam. Adam uses a combination of RMSprop and Momentum to optimize the gradient descent. Therefore, something about the dataset preferred RMSprop's method of gradient descent. This adjustment lead to the biggest increase in accuracy.

```{r}
# Load Dataset
library(keras)
library(tensorflow)

cifar <- dataset_cifar10()

train_X <- cifar$train$x
train_y <- cifar$train$y

test_X <- cifar$test$x
test_y <- cifar$test$y
```

```{r}
set.seed(14)
# Define Model From Lecture
relu_model <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "relu", 
                input_shape = c(32,32,3)) %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "relu") %>% 
  layer_flatten() %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 10, activation = "softmax")

summary(relu_model)

relu_model %>% compile(
  optimizer = "adam",
  loss = "sparse_categorical_crossentropy",
  metrics = "accuracy"
)
```
```{r}
set.seed(14)
# Training from lecture
history <- relu_model %>% 
  fit(
    x = cifar$train$x, y = cifar$train$y,
    epochs = 10,
    validation_data = unname(cifar$test),
    verbose = 2
  )

# Plot from lecture
plot(history)

# Output from lecture
evaluate(relu_model, cifar$test$x, cifar$test$y, verbose = 0)

```

```{r}
set.seed(14)
# Trying elu model (IMPROVED!)

model <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "elu", 
                input_shape = c(32,32,3)) %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "elu") %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "elu") %>% 
  layer_flatten() %>% 
  layer_dense(units = 64, activation = "elu") %>% 
  layer_dense(units = 10, activation = "softmax") 

summary(model)

model %>% compile(
  optimizer = "adam",
  loss = "sparse_categorical_crossentropy",
  metrics = "accuracy"
)
```
```{r}
set.seed(14)
# Training ELU (changed optimizer)
history <- model %>% 
  fit(
    x = cifar$train$x, y = cifar$train$y,
    epochs = 10,
    validation_data = unname(cifar$test),
    verbose = 2
  )

# Plot from lecture
plot(history)

# Output from lecture
evaluate(model, cifar$test$x, cifar$test$y, verbose = 0)

```
```{r}
set.seed(14)
# elu with changed optimizer (better!) 
model %>% compile(
  optimizer = "RMSprop",
  loss = "sparse_categorical_crossentropy",
  metrics = "accuracy"
)

history <- model %>% 
  fit(
    x = cifar$train$x, y = cifar$train$y,
    epochs = 10,
    validation_data = unname(cifar$test),
    verbose = 2
  )

# Plot from lecture
plot(history)

# Output from lecture
evaluate(model, cifar$test$x, cifar$test$y, verbose = 0)

```
**The test set accuracy of my best model (elu and RMSprop), can be seen above. As you can see from the three iterations I've run above, the loss is lower in the first two iterations, but the accuracy is also lower. This is likely because the first two models are overfitting to the train data, creating lower loss, but worse out of train sample accuracy.**

**I found that the biggest changes in this dataset came from changing the optimizing functions, rather than the actual makeup of the neural network. This is not to say that the framework of a neural network does not matter. Since I started my changes off of the model from lecture, it is likely to assume that this model was already relatively optimized for the dataset, which is why changing the number of layers and filters did not improve the accuracy. Furthermore, it makes sense that changing the activation and optimizing functions had such a big impact, because as we learned in lecture, the move of neural networks from sigmoid to relu/elu models was a large reason for their resurgence in modern day deep learning discourse. Furthermore, it makes sense that finding the best function for a dataset would have a large impact on its performance. Something that would improve the result of the accuracy would be training it for more epochs. I had to lower the epochs to 5 during training because of computational and time constraints. For the 3 iterations which I have kept to demonstrate the improvements I found, I increased the epochs back to 10.**

#### **EVERYTHING BELOW HAS LOWER ACCURACY**

#### Elu with Adagrad
model %>% compile(
  optimizer = "Adagrad",
  loss = "sparse_categorical_crossentropy",
  metrics = "accuracy"
)

history <- model %>% 
  fit(
    x = cifar$train$x, y = cifar$train$y,
    epochs = 5,
    validation_data = unname(cifar$test),
    verbose = 2
  )


#### add a layer in 

model <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "elu", 
                input_shape = c(32,32,3)) %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "elu") %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "elu") %>%
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "elu") %>% 
  layer_flatten() %>% 
  layer_dense(units = 64, activation = "elu") %>% 
  layer_dense(units = 10, activation = "softmax")

model %>% compile(
  optimizer = "RMSprop",
  loss = "sparse_categorical_crossentropy",
  metrics = "accuracy"
)

#### change filter

model <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "elu", 
                input_shape = c(32,32,3)) %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "elu") %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3,3), activation = "elu") %>% 
  layer_flatten() %>% 
  layer_dense(units = 64, activation = "elu") %>% 
  layer_dense(units = 10, activation = "softmax") 

model %>% compile(
  optimizer = "RMSprop",
  loss = "sparse_categorical_crossentropy",
  metrics = "accuracy"
)

#### sigmoid, better than RELU, not ELU

model <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "sigmoid", 
                input_shape = c(32,32,3)) %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "sigmoid") %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "sigmoid") %>% 
  layer_flatten() %>% 
  layer_dense(units = 64, activation = "sigmoid") %>% 
  layer_dense(units = 10, activation = "softmax") 

summary(model)

model %>% compile(
  optimizer = "adam",
  loss = "sparse_categorical_crossentropy",
  metrics = "accuracy"
)

#### Changing kernel sizes to 5,5

model <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "elu", 
                input_shape = c(32,32,3)) %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(5,5), activation = "elu") %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(5,5), activation = "elu") %>% 
  layer_flatten() %>% 
  layer_dense(units = 64, activation = "elu") %>% 
  layer_dense(units = 10, activation = "softmax") 

#### Changing kernel sizes to 2,2

model <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "elu", 
                input_shape = c(32,32,3)) %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(2,2), activation = "elu") %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(2,2), activation = "elu") %>% 
  layer_flatten() %>% 
  layer_dense(units = 64, activation = "elu") %>% 
  layer_dense(units = 10, activation = "softmax") 

#### Asymmetric kernels

model <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "elu", 
                input_shape = c(32,32,3)) %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,5), activation = "elu") %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,5), activation = "elu") %>% 
  layer_flatten() %>% 
  layer_dense(units = 64, activation = "elu") %>% 
  layer_dense(units = 10, activation = "softmax") 

summary(model)

model %>% compile(
  optimizer = "adam",
  loss = "sparse_categorical_crossentropy",
  metrics = "accuracy"
)

#### Changing kernel sizes to 2,2

model <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "elu", 
                input_shape = c(32,32,3)) %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(4,4), activation = "elu") %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(4,4), activation = "elu") %>% 
  layer_flatten() %>% 
  layer_dense(units = 64, activation = "elu") %>% 
  layer_dense(units = 10, activation = "softmax") 

